{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEEuCAL4FGJA"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuiL-1xlo1vP"
   },
   "source": [
    "The Test Dataset is used to compare our implementations with those from sklearn. The intent is to achieve equality after vectorization. To mimic the results from sklearn, the vectorizers also do very basic cleaing. They replace all punctuation with spaces, and all characters become lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJ_MSpFDAMLn"
   },
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "c1 = 'Human machine interface for Lab ABC computer applications'\n",
    "c2 = 'A survey of user opinion of computer system response time'\n",
    "c3 = 'The EPS user interface management system'\n",
    "c4 = 'System and human system engineering testing of EPS'\n",
    "c5 = 'Relation of user-perceived response time to error measurement'\n",
    "m1 = 'The generation of random, binary, unordered trees'\n",
    "m2 = 'The intersection graph of paths in trees'\n",
    "m3 = 'Graph minors IV: Widths of trees and well-quasi-ordering'\n",
    "m4 = 'Graph minors: A survey'\n",
    "documents = [c1, c2, c3, c4, c5, m1, m2, m3, m4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bK_T59dwJ9k1"
   },
   "outputs": [],
   "source": [
    "class CustomCountVectorizer:\n",
    "  def create_vectors(self, documents):\n",
    "    # Get Vocab\n",
    "    vocab = set()\n",
    "    clean_docs = list()\n",
    "    for doc in documents:\n",
    "      lower_doc = doc.lower()\n",
    "      lower_doc_no_sym = ''.join([i if i not in string.punctuation else ' ' for i in lower_doc])\n",
    "      doc_arr = lower_doc_no_sym.split()\n",
    "      clean_docs.append(doc_arr)\n",
    "      vocab = vocab.union(doc_arr)\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    # Get Counts\n",
    "    vectors = np.zeros((len(clean_docs), len(vocab)))\n",
    "    for doc_idx, doc in enumerate(clean_docs):\n",
    "      for wrd_idx, word in enumerate(vocab):\n",
    "        count = doc.count(word)\n",
    "        vectors[doc_idx][wrd_idx] = count\n",
    "    \n",
    "    return vectors.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Q--uoAiN34T",
    "outputId": "8772c13b-dff0-4179-afa2-a3e1d571d010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# By default, CountVectorizer from sklearn ignores tokens under length 2\n",
    "control_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "control_vecs = control_vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "custom_vectorizer = CustomCountVectorizer()\n",
    "custom_vecs = custom_vectorizer.create_vectors(documents)\n",
    "print((control_vecs == custom_vecs).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDgbFKOgiL7-"
   },
   "outputs": [],
   "source": [
    "class CustomTFIDFVectorizer:\n",
    "  def create_vectors(self, documents):\n",
    "    vocab = set()\n",
    "    clean_docs = list()\n",
    "    for doc in documents:\n",
    "      lower_doc = doc.lower()\n",
    "      lower_doc_no_sym = ''.join([i if i not in string.punctuation else ' ' for i in lower_doc])\n",
    "      doc_arr = lower_doc_no_sym.split()\n",
    "      clean_docs.append(doc_arr)\n",
    "      vocab = vocab.union(doc_arr)\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    idf_dict = {}\n",
    "    for word in vocab:\n",
    "      count = sum(1 for document in clean_docs if word in document)\n",
    "      idf_dict[word] = 1 + np.log((len(clean_docs) + 1) / (count + 1))\n",
    "\n",
    "    vectors = np.zeros((len(clean_docs), len(vocab)))\n",
    "    for doc_idx, doc in enumerate(clean_docs):\n",
    "      for wrd_idx, word in enumerate(vocab):\n",
    "        tf = doc.count(word) / len(doc)\n",
    "        idf = idf_dict[word]\n",
    "        vectors[doc_idx][wrd_idx] = tf * idf\n",
    "\n",
    "    return preprocessing.normalize(vectors, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQDNPyEgij9Y",
    "outputId": "ee719154-ca90-4ab5-dea7-0c6b4b9ce2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "control_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "control_vecs = control_vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "custom_vectorizer = CustomTFIDFVectorizer()\n",
    "custom_vecs = custom_vectorizer.create_vectors(documents)\n",
    "print(np.allclose(control_vecs, custom_vecs, atol=1e-14))\n",
    "# Due to rounding, we compare for near equality instead"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Count and TFIDF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dce57450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4de319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy deals with large arrays and linear algebra\n",
    "import numpy as np\n",
    "# Library for data manipulation and analysis\n",
    "import pandas as pd \n",
    " \n",
    "# Metrics for Evaluation of model Accuracy and F1-score\n",
    "from sklearn.metrics  import f1_score, accuracy_score\n",
    " \n",
    "# Importing the Decision Tree from scikit-learn library.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Regression based Classifier.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Naive Bayes Models.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# # Stochastic Gradient Descent classifier .\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Distance based Classifier.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Ensemble models.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    " \n",
    "# For splitting of data into train and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84074eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a1e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open(r'S:\\MS AI NEU\\CS 6120 - NLP\\NLP_Project\\primes1\\primes1.txt', 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "369e3f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                 The First 1,000,000 Primes (from primes.utm.edu)\\n',\n",
       " '\\n',\n",
       " '         2         3         5         7        11        13        17        19 \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6cf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "primes_list = [y for x in lines[2:] for y in f\"{x.strip()}\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383664ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '3', '5', '7', '11', '13', '17', '19', '23', '29']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "primes_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34ddd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ce051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(rf'Train_Data.csv')\n",
    "df_test = pd.read_csv(rf'Test_Data.csv')\n",
    "\n",
    "vocab = {x for y in df['Description'] for x in y.strip().split()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7f203af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rockwel',\n",
       " 'rexi',\n",
       " 'copycat',\n",
       " 'gavric',\n",
       " 'strikebreak',\n",
       " 'linson',\n",
       " 'montrassor',\n",
       " 'raina',\n",
       " 'kalevi',\n",
       " 'brechtian',\n",
       " 'blackwel',\n",
       " 'utah',\n",
       " 'manip',\n",
       " 'concert',\n",
       " 'thrust',\n",
       " 'earhart',\n",
       " 'viol',\n",
       " 'cardiologist',\n",
       " 'earlham',\n",
       " 'quarant',\n",
       " 'teme',\n",
       " 'melilla',\n",
       " 'hilar',\n",
       " 'salley',\n",
       " 'abad',\n",
       " 'nanobot',\n",
       " 'plot',\n",
       " 'fucsia',\n",
       " 'cueva',\n",
       " 'phenominon',\n",
       " 'anastasi',\n",
       " 'winnebago',\n",
       " 'bonk',\n",
       " 'spahn',\n",
       " 'coelho',\n",
       " 'pteranodon',\n",
       " 'qanta',\n",
       " 'omnisci',\n",
       " 'turopolj',\n",
       " 'cyteck',\n",
       " 'incarnazion',\n",
       " 'wajda',\n",
       " 'quizzic',\n",
       " 'cuchillo',\n",
       " 'feder',\n",
       " 'weigand',\n",
       " 'leukerbad',\n",
       " 'joong',\n",
       " 'dena',\n",
       " 'montogomeri',\n",
       " 'trilog',\n",
       " 'bhaat',\n",
       " 'kislanyav',\n",
       " 'flem',\n",
       " 'loyer',\n",
       " 'trawl',\n",
       " 'communic',\n",
       " 'vulva',\n",
       " 'rastafarian',\n",
       " 'mangl',\n",
       " 'kinderheim',\n",
       " 'bilt',\n",
       " 'unbefit',\n",
       " 'leukemia',\n",
       " 'waist',\n",
       " 'schreck',\n",
       " 'fembl',\n",
       " 'drachma',\n",
       " 'stinson',\n",
       " 'zirpollo',\n",
       " 'bhankya',\n",
       " 'labrador',\n",
       " 'jorett',\n",
       " 'filtrat',\n",
       " 'mystiqu',\n",
       " 'frihet',\n",
       " 'vivisectionist',\n",
       " 'pothineni',\n",
       " 'jlon',\n",
       " 'evat',\n",
       " 'arnolph',\n",
       " 'ilon',\n",
       " 'handler',\n",
       " 'sosia',\n",
       " 'clownhous',\n",
       " 'desktop',\n",
       " 'tiferet',\n",
       " 'narayanan',\n",
       " 'asbo',\n",
       " 'diederen',\n",
       " 'unquiet',\n",
       " 'baldev',\n",
       " 'melford',\n",
       " 'gibberish',\n",
       " 'attard',\n",
       " 'longbow',\n",
       " 'feelin',\n",
       " 'wum',\n",
       " 'achu',\n",
       " 'conspicu',\n",
       " 'rinehart',\n",
       " 'atkin',\n",
       " 'conden',\n",
       " 'bakul',\n",
       " 'armonk',\n",
       " 'burnley',\n",
       " 'valley',\n",
       " 'raaz',\n",
       " 'corll',\n",
       " 'frehel',\n",
       " 'micki',\n",
       " 'ene',\n",
       " 'farrukhabad',\n",
       " 'ariola',\n",
       " 'hmmm',\n",
       " 'shipwreck',\n",
       " 'ttan',\n",
       " 'bricklay',\n",
       " 'zhuan',\n",
       " 'citigroup',\n",
       " 'revri',\n",
       " 'conting',\n",
       " 'puneet',\n",
       " 'brynner',\n",
       " 'remnick',\n",
       " 'tchechenko',\n",
       " 'navicula',\n",
       " 'organizado',\n",
       " 'filmrendezoknek',\n",
       " 'unsoci',\n",
       " 'sulochana',\n",
       " 'dilut',\n",
       " 'setareh',\n",
       " 'mcneill',\n",
       " 'mileag',\n",
       " 'dowtin',\n",
       " 'km',\n",
       " 'tidliger',\n",
       " 'tourvain',\n",
       " 'sezgin',\n",
       " 'currentv',\n",
       " 'cinemax',\n",
       " 'kendama',\n",
       " 'mohlk',\n",
       " 'pilli',\n",
       " 'toxic',\n",
       " 'falingard',\n",
       " 'cherbourg',\n",
       " 'mieduniszki',\n",
       " 'vahradian',\n",
       " 'bumdog',\n",
       " 'morgan',\n",
       " 'sydni',\n",
       " 'dangerzon',\n",
       " 'filmland',\n",
       " 'daphn',\n",
       " 'cohabit',\n",
       " 'phandey',\n",
       " 'contegno',\n",
       " 'rever',\n",
       " 'riment',\n",
       " 'pipi',\n",
       " 'mkhize',\n",
       " 'schakowski',\n",
       " 'bronzevil',\n",
       " 'schoolbook',\n",
       " 'gat',\n",
       " 'fagan',\n",
       " 'vagina',\n",
       " 'obergruppehfuhr',\n",
       " 'farzad',\n",
       " 'dietro',\n",
       " 'dhangao',\n",
       " 'servicewoman',\n",
       " 'reddick',\n",
       " 'howard',\n",
       " 'mahua',\n",
       " 'autom',\n",
       " 'giraffestonburi',\n",
       " 'varco',\n",
       " 'swinem',\n",
       " 'pinkpop',\n",
       " 'flasher',\n",
       " 'freddi',\n",
       " 'hoeferlin',\n",
       " 'edmundo',\n",
       " 'rakhwala',\n",
       " 'elmira',\n",
       " 'unedit',\n",
       " 'encontrar',\n",
       " 'franey',\n",
       " 'sheep',\n",
       " 'norfolk',\n",
       " 'sling',\n",
       " 'honkong',\n",
       " 'kahoy',\n",
       " 'maneuv',\n",
       " 'pauhofov',\n",
       " 'csr',\n",
       " 'moonskul',\n",
       " 'wittelsbach',\n",
       " 'complot',\n",
       " 'yuli',\n",
       " 'broek',\n",
       " 'ramingin',\n",
       " 'pawlusiak',\n",
       " 'photograhp',\n",
       " 'strike',\n",
       " 'yuba',\n",
       " 'confus',\n",
       " 'roni',\n",
       " 'briem',\n",
       " 'bagarella',\n",
       " 'acromegali',\n",
       " 'coomb',\n",
       " 'sabadel',\n",
       " 'sprynczynatyk',\n",
       " 'brandon',\n",
       " 'pallett',\n",
       " 'vallenato',\n",
       " 'nascent',\n",
       " 'verbinski',\n",
       " 'kamar',\n",
       " 'yame',\n",
       " 'daybreak',\n",
       " 'disgruntl',\n",
       " 'organilum',\n",
       " 'fishbon',\n",
       " 'provocateur',\n",
       " 'drezner',\n",
       " 'soju',\n",
       " 'vc',\n",
       " 'hersh',\n",
       " 'ema',\n",
       " 'peski',\n",
       " 'perish',\n",
       " 'dmitriy',\n",
       " 'wharf',\n",
       " 'greengown',\n",
       " 'crous',\n",
       " 'gobernado',\n",
       " 'chizik',\n",
       " 'countervail',\n",
       " 'em',\n",
       " 'campeau',\n",
       " 'mingtao',\n",
       " 'lamberto',\n",
       " 'vinaya',\n",
       " 'meehan',\n",
       " 'pinehurst',\n",
       " 'klee',\n",
       " 'mabley',\n",
       " 'bernes',\n",
       " 'naismith',\n",
       " 'skyro',\n",
       " 'jalopi',\n",
       " 'erebonia',\n",
       " 'nje',\n",
       " 'tayfun',\n",
       " 'tror',\n",
       " 'lifeforc',\n",
       " 'famliy',\n",
       " 'iraqu',\n",
       " 'australasia',\n",
       " 'roqu',\n",
       " 'jacki',\n",
       " 'anc',\n",
       " 'martinez',\n",
       " 'romualdo',\n",
       " 'enumer',\n",
       " 'saanvri',\n",
       " 'pict',\n",
       " 'utahn',\n",
       " 'zahi',\n",
       " 'plea',\n",
       " 'teaneck',\n",
       " 'umrao',\n",
       " 'kaz',\n",
       " 'cleci',\n",
       " 'riggo',\n",
       " 'gamin',\n",
       " 'edlund',\n",
       " 'stoya',\n",
       " 'lavinth',\n",
       " 'nutritionist',\n",
       " 'bonanza',\n",
       " 'camion',\n",
       " 'trestman',\n",
       " 'megvan',\n",
       " 'sigismond',\n",
       " 'quilt',\n",
       " 'ipojuca',\n",
       " 'astor',\n",
       " 'shard',\n",
       " 'maharashtrian',\n",
       " 'statsr',\n",
       " 'aguerr',\n",
       " 'ning',\n",
       " 'hollingsworth',\n",
       " 'twee',\n",
       " 'olip',\n",
       " 'yousra',\n",
       " 'kwong',\n",
       " 'hershel',\n",
       " 'bonati',\n",
       " 'psychedel',\n",
       " 'rajaram',\n",
       " 'latelin',\n",
       " 'pati',\n",
       " 'zakliczyn',\n",
       " 'suezann',\n",
       " 'conjur',\n",
       " 'baren',\n",
       " 'maneng',\n",
       " 'ackerley',\n",
       " 'unwit',\n",
       " 'nekichi',\n",
       " 'sammon',\n",
       " 'rnklo',\n",
       " 'canneboi',\n",
       " 'ohioan',\n",
       " 'transauru',\n",
       " 'helzberg',\n",
       " 'singularli',\n",
       " 'saiful',\n",
       " 'nalanda',\n",
       " 'narudom',\n",
       " 'redstaff',\n",
       " 'kore',\n",
       " 'suspensjon',\n",
       " 'topi',\n",
       " 'lovewel',\n",
       " 'huub',\n",
       " 'barg',\n",
       " 'sanlap',\n",
       " 'wfbq',\n",
       " 'parmeshwar',\n",
       " 'himm',\n",
       " 'sardonia',\n",
       " 'killigan',\n",
       " 'ancient',\n",
       " 'feebli',\n",
       " 'wiand',\n",
       " 'jeanni',\n",
       " 'viktor',\n",
       " 'stipan',\n",
       " 'kawintseb',\n",
       " 'ruccolo',\n",
       " 'mcintosh',\n",
       " 'condominio',\n",
       " 'ioseb',\n",
       " 'boyer',\n",
       " 'fucke',\n",
       " 'ooga',\n",
       " 'schtick',\n",
       " 'waltz',\n",
       " 'laalaa',\n",
       " 'agelo',\n",
       " 'hasten',\n",
       " 'merav',\n",
       " 'worrisom',\n",
       " 'tigh',\n",
       " 'caomei',\n",
       " 'extract',\n",
       " 'risalir',\n",
       " 'mucu',\n",
       " 'hash',\n",
       " 'rosaria',\n",
       " 'barbro',\n",
       " 'rajakumaran',\n",
       " 'zlatko',\n",
       " 'cantautor',\n",
       " 'vike',\n",
       " 'scuttl',\n",
       " 'ayu',\n",
       " 'luckiest',\n",
       " 'glauco',\n",
       " 'blyton',\n",
       " 'georgina',\n",
       " 'belair',\n",
       " 'grata',\n",
       " 'interraci',\n",
       " 'nsumba',\n",
       " 'licier',\n",
       " 'ranc',\n",
       " 'paltri',\n",
       " 'noor',\n",
       " 'nondescript',\n",
       " 'ranjot',\n",
       " 'screwup',\n",
       " 'happin',\n",
       " 'append',\n",
       " 'brief',\n",
       " 'ejacul',\n",
       " 'relacionar',\n",
       " 'loav',\n",
       " 'ponzanelli',\n",
       " 'riconquista',\n",
       " 'ranikhet',\n",
       " 'mitchellini',\n",
       " 'mise',\n",
       " 'zeeland',\n",
       " 'cbd',\n",
       " 'reencount',\n",
       " 'laurett',\n",
       " 'spatiotempor',\n",
       " 'timberland',\n",
       " 'apx',\n",
       " 'quebeck',\n",
       " 'fergal',\n",
       " 'aao',\n",
       " 'avventuroso',\n",
       " 'witkin',\n",
       " 'middlebrook',\n",
       " 'milisa',\n",
       " 'dentist',\n",
       " 'regalia',\n",
       " 'cliffhang',\n",
       " 'asperg',\n",
       " 'corman',\n",
       " 'cognomen',\n",
       " 'sidoni',\n",
       " 'boynton',\n",
       " 'yuzu',\n",
       " 'philantroph',\n",
       " 'mellow',\n",
       " 'ax',\n",
       " 'klu',\n",
       " 'zhentong',\n",
       " 'shaki',\n",
       " 'abrahamian',\n",
       " 'phrakhanong',\n",
       " 'desert',\n",
       " 'odham',\n",
       " 'harir',\n",
       " 'pustul',\n",
       " 'trudel',\n",
       " 'ranchidiari',\n",
       " 'popcitrouillard',\n",
       " 'woon',\n",
       " 'gro',\n",
       " 'arabl',\n",
       " 'nicholaa',\n",
       " 'ft',\n",
       " 'filim',\n",
       " 'pole',\n",
       " 'ellison',\n",
       " 'crerand',\n",
       " 'lamppost',\n",
       " 'pend',\n",
       " 'unrequest',\n",
       " 'ballestero',\n",
       " 'deari',\n",
       " 'incompet',\n",
       " 'betingels',\n",
       " 'stepp',\n",
       " 'mouro',\n",
       " 'rodley',\n",
       " 'turtlelik',\n",
       " 'pretexto',\n",
       " 'poobah',\n",
       " 'r',\n",
       " 'trevor',\n",
       " 'clamid',\n",
       " 'scheveningen',\n",
       " 'sakhai',\n",
       " 'cheatham',\n",
       " 'ainley',\n",
       " 'gelijk',\n",
       " 'hidekazu',\n",
       " 'icinden',\n",
       " 'conservadora',\n",
       " 'sultanahmet',\n",
       " 'sentebal',\n",
       " 'pregnanti',\n",
       " 'kheyang',\n",
       " 'servitud',\n",
       " 'markrut',\n",
       " 'aimonid',\n",
       " 'hassid',\n",
       " 'veni',\n",
       " 'bhakt',\n",
       " 'astronomi',\n",
       " 'wenraub',\n",
       " 'nightingal',\n",
       " 'eyewit',\n",
       " 'whimsi',\n",
       " 'dangereus',\n",
       " 'laprad',\n",
       " 'quinn',\n",
       " 'wwf',\n",
       " 'fidanzata',\n",
       " 'seton',\n",
       " 'quindici',\n",
       " 'spang',\n",
       " 'agath',\n",
       " 'malatesta',\n",
       " 'milennium',\n",
       " 'tsgt',\n",
       " 'loiseau',\n",
       " 'wondervil',\n",
       " 'kibriya',\n",
       " 'seventh',\n",
       " 'upend',\n",
       " 'robeat',\n",
       " 'downlin',\n",
       " 'deton',\n",
       " 'braswel',\n",
       " 'viay',\n",
       " 'carton',\n",
       " 'naya',\n",
       " 'andert',\n",
       " 'citybal',\n",
       " 'petroleum',\n",
       " 'form',\n",
       " 'milliomoslani',\n",
       " 'alphavil',\n",
       " 'illus',\n",
       " 'spooker',\n",
       " 'brandao',\n",
       " 'firearm',\n",
       " 'nightshad',\n",
       " 'hemmer',\n",
       " 'slalom',\n",
       " 'gangubai',\n",
       " 'patienc',\n",
       " 'unexploit',\n",
       " 'techniqu',\n",
       " 'fitzhew',\n",
       " 'francophon',\n",
       " 'ingar',\n",
       " 'pauli',\n",
       " 'delong',\n",
       " 'inesistent',\n",
       " 'ricoul',\n",
       " 'picker',\n",
       " 'veneer',\n",
       " 'zinger',\n",
       " 'stag',\n",
       " 'carl',\n",
       " 'pappert',\n",
       " 'decemb',\n",
       " 'impercept',\n",
       " 'gambk',\n",
       " 'albright',\n",
       " 'bearwalk',\n",
       " 'estimul',\n",
       " 'kunthalgirikar',\n",
       " 'scrat',\n",
       " 'fripp',\n",
       " 'pantan',\n",
       " 'colerain',\n",
       " 'superdawg',\n",
       " 'magnif',\n",
       " 'pua',\n",
       " 'zbigniew',\n",
       " 'pox',\n",
       " 'kuvukiland',\n",
       " 'puccini',\n",
       " 'umm',\n",
       " 'michalski',\n",
       " 'zoraida',\n",
       " 'bourgueois',\n",
       " 'ohanian',\n",
       " 'escondido',\n",
       " 'fioril',\n",
       " 'flagstaff',\n",
       " 'cpasa',\n",
       " 'cyberterror',\n",
       " 'vibhishan',\n",
       " 'sulder',\n",
       " 'yub',\n",
       " 'veldleg',\n",
       " 'board',\n",
       " 'apiec',\n",
       " 'dadar',\n",
       " 'curios',\n",
       " 'balk',\n",
       " 'alessio',\n",
       " 'filmfestiv',\n",
       " 'aeg',\n",
       " 'syriam',\n",
       " 'nobl',\n",
       " 'yarn',\n",
       " 'mzee',\n",
       " 'kardna',\n",
       " 'veysel',\n",
       " 'erudit',\n",
       " 'multipli',\n",
       " 'admit',\n",
       " 'nearsight',\n",
       " 'harrass',\n",
       " 'heighten',\n",
       " 'kyrgyz',\n",
       " 'gunvor',\n",
       " 'falsehood',\n",
       " 'vinton',\n",
       " 'darrah',\n",
       " 'zahir',\n",
       " 'raad',\n",
       " 'forough',\n",
       " 'subordin',\n",
       " 'leng',\n",
       " 'trouper',\n",
       " 'abitata',\n",
       " 'constan',\n",
       " 'ukeir',\n",
       " 'amazingli',\n",
       " 'flatb',\n",
       " 'concordia',\n",
       " 'voyeurism',\n",
       " 'sou',\n",
       " 'panggabean',\n",
       " 'unsuccess',\n",
       " 'serki',\n",
       " 'payless',\n",
       " 'spyder',\n",
       " 'pard',\n",
       " 'rysher',\n",
       " 'voglia',\n",
       " 'ekiz',\n",
       " 'chiquita',\n",
       " 'waxahachi',\n",
       " 'innov',\n",
       " 'minel',\n",
       " 'impersn',\n",
       " 'dorre',\n",
       " 'cloudless',\n",
       " 'rydel',\n",
       " 'tiina',\n",
       " 'weeklong',\n",
       " 'osnabrueck',\n",
       " 'injunct',\n",
       " 'jayasudha',\n",
       " 'milligan',\n",
       " 'sit',\n",
       " 'falter',\n",
       " 'callout',\n",
       " 'calma',\n",
       " 'ova',\n",
       " 'mccarran',\n",
       " 'ragosin',\n",
       " 'adhaobi',\n",
       " 'cavern',\n",
       " 'winnemac',\n",
       " 'shahrokh',\n",
       " 'rie',\n",
       " 'midfield',\n",
       " 'moos',\n",
       " 'iura',\n",
       " 'yusko',\n",
       " 'twitter',\n",
       " 'hyapatia',\n",
       " 'belden',\n",
       " 'havel',\n",
       " 'downpour',\n",
       " 'voynic',\n",
       " 'noto',\n",
       " 'archeologist',\n",
       " 'eyruv',\n",
       " 'russullella',\n",
       " 'natacia',\n",
       " 'outdoorsi',\n",
       " 'manab',\n",
       " 'soonest',\n",
       " 'bubblin',\n",
       " 'arff',\n",
       " 'letta',\n",
       " 'capter',\n",
       " 'cobanoglu',\n",
       " 'chansonni',\n",
       " 'disharmoni',\n",
       " 'midterm',\n",
       " 'burdeni',\n",
       " 'elasmobranch',\n",
       " 'kenyszerul',\n",
       " 'socki',\n",
       " 'garibda',\n",
       " 'shanika',\n",
       " 'maggin',\n",
       " 'lovetalk',\n",
       " 'spitfir',\n",
       " 'shite',\n",
       " 'batson',\n",
       " 'maier',\n",
       " 'scapegrac',\n",
       " 'guillam',\n",
       " 'televison',\n",
       " 'gritz',\n",
       " 'agnesa',\n",
       " 'soundtrack',\n",
       " 'caldo',\n",
       " 'szerepl',\n",
       " 'gangapaddhaya',\n",
       " 'xoel',\n",
       " 'kreat',\n",
       " 'baldra',\n",
       " 'cormoran',\n",
       " 'workingmen',\n",
       " 'fulco',\n",
       " 'misdemeanor',\n",
       " 'overdos',\n",
       " 'mesmerist',\n",
       " 'mugulu',\n",
       " 'senador',\n",
       " 'sciascia',\n",
       " 'mayrhofen',\n",
       " 'zijn',\n",
       " 'steppar',\n",
       " 'mcloughlin',\n",
       " 'ske',\n",
       " 'maat',\n",
       " 'lija',\n",
       " 'okeechobe',\n",
       " 'paddington',\n",
       " 'acropoli',\n",
       " 'promiscu',\n",
       " 'pupillo',\n",
       " 'nephi',\n",
       " 'parola',\n",
       " 'pannonian',\n",
       " 'thalassemia',\n",
       " 'dimitrieski',\n",
       " 'curtin',\n",
       " 'ichabod',\n",
       " 'crimp',\n",
       " 'winnabl',\n",
       " 'industria',\n",
       " 'sumeria',\n",
       " 'eri',\n",
       " 'phraseolog',\n",
       " 'ceux',\n",
       " 'exceris',\n",
       " 'chong',\n",
       " 'roadster',\n",
       " 'tb',\n",
       " 'douval',\n",
       " 'porzig',\n",
       " 'assolutament',\n",
       " 'schlumberg',\n",
       " 'budz',\n",
       " 'hindu',\n",
       " 'montegnya',\n",
       " 'introduc',\n",
       " 'neosoul',\n",
       " 'tiberiad',\n",
       " 'ballist',\n",
       " 'dee',\n",
       " 'bonta',\n",
       " 'mientj',\n",
       " 'bechor',\n",
       " 'jitu',\n",
       " 'wozzeck',\n",
       " 'schrute',\n",
       " 'rancher',\n",
       " 'elat',\n",
       " 'innamora',\n",
       " 'birk',\n",
       " 'niner',\n",
       " 'rauch',\n",
       " 'carolinian',\n",
       " 'garona',\n",
       " 'kenin',\n",
       " 'sadhavi',\n",
       " 'manri',\n",
       " 'eleusina',\n",
       " 'cheeseburg',\n",
       " 'avedissyan',\n",
       " 'bunti',\n",
       " 'tikbalang',\n",
       " 'administr',\n",
       " 'basilio',\n",
       " 'zadar',\n",
       " 'anant',\n",
       " 'infact',\n",
       " 'spenzo',\n",
       " 'forum',\n",
       " 'msp',\n",
       " 'manasiev',\n",
       " 'tun',\n",
       " 'beret',\n",
       " 'vijetha',\n",
       " 'davinci',\n",
       " 'district',\n",
       " 'gartner',\n",
       " 'pape',\n",
       " 'leijoncloo',\n",
       " 'stenwal',\n",
       " 'shakespearan',\n",
       " 'proto',\n",
       " 'pendleton',\n",
       " 'zaina',\n",
       " 'pournari',\n",
       " 'urmi',\n",
       " 'corpir',\n",
       " 'neurodivers',\n",
       " 'preform',\n",
       " 'bedan',\n",
       " 'sprinkler',\n",
       " 'lazili',\n",
       " 'psichiatra',\n",
       " 'gundappa',\n",
       " 'zerneck',\n",
       " 'cancerv',\n",
       " 'hyperdr',\n",
       " 'irena',\n",
       " 'someday',\n",
       " 'roux',\n",
       " 'photograp',\n",
       " 'shamba',\n",
       " 'jingju',\n",
       " 'phase',\n",
       " 'chihiro',\n",
       " 'urologist',\n",
       " 'quattro',\n",
       " 'odsaluyang',\n",
       " 'objetivo',\n",
       " 'paniek',\n",
       " 'lanzamiento',\n",
       " 'venedikt',\n",
       " 'khagrachari',\n",
       " 'erebor',\n",
       " 'palawan',\n",
       " 'ridotta',\n",
       " 'tetuan',\n",
       " 'korridor',\n",
       " 'zip',\n",
       " 'rattar',\n",
       " 'pritchett',\n",
       " 'luigi',\n",
       " 'ceil',\n",
       " 'partendo',\n",
       " 'gorin',\n",
       " 'ev',\n",
       " 'holodeck',\n",
       " 'matsumura',\n",
       " 'commodifi',\n",
       " 'timeclock',\n",
       " 'focos',\n",
       " 'hsam',\n",
       " 'domina',\n",
       " 'jatekosait',\n",
       " 'rangeland',\n",
       " 'lam',\n",
       " 'maillion',\n",
       " 'khanom',\n",
       " 'coot',\n",
       " 'palomo',\n",
       " 'agronom',\n",
       " 'unseen',\n",
       " 'causar',\n",
       " 'hattendorf',\n",
       " 'reader',\n",
       " 'cinemascop',\n",
       " 'jianch',\n",
       " 'condemn',\n",
       " 'grandag',\n",
       " 'baleset',\n",
       " 'roghieh',\n",
       " 'baggat',\n",
       " 'abbassi',\n",
       " 'hado',\n",
       " 'uncoop',\n",
       " 'farrelli',\n",
       " 'moller',\n",
       " 'roosevelt',\n",
       " 'fundo',\n",
       " 'srintil',\n",
       " 'bagher',\n",
       " 'koblovski',\n",
       " 'concess',\n",
       " 'manbij',\n",
       " 'insist',\n",
       " 'ezeki',\n",
       " 'chachita',\n",
       " 'pompou',\n",
       " 'tangi',\n",
       " 'toyah',\n",
       " 'painless',\n",
       " 'martiro',\n",
       " 'premis',\n",
       " 'uav',\n",
       " 'apuania',\n",
       " 'comet',\n",
       " 'codears',\n",
       " 'protestata',\n",
       " 'duddridg',\n",
       " 'bomsta',\n",
       " 'satara',\n",
       " 'gatekeep',\n",
       " 'kiswahili',\n",
       " 'diventi',\n",
       " 'ard',\n",
       " 'korbel',\n",
       " 'yuko',\n",
       " 'fitzjohn',\n",
       " 'lescaut',\n",
       " 'elhatarozza',\n",
       " 'oermann',\n",
       " 'conman',\n",
       " 'condal',\n",
       " 'rilow',\n",
       " 'fein',\n",
       " 'kesa',\n",
       " 'coce',\n",
       " 'cruis',\n",
       " 'jemma',\n",
       " 'lidster',\n",
       " 'nadejda',\n",
       " 'spectateur',\n",
       " 'sytl',\n",
       " 'shahidi',\n",
       " 'dartmouth',\n",
       " 'loucka',\n",
       " 'balan',\n",
       " 'bourrel',\n",
       " 'tanba',\n",
       " 'zaidian',\n",
       " 'baku',\n",
       " 'hubler',\n",
       " 'activis',\n",
       " 'overtak',\n",
       " 'alesia',\n",
       " 'namrata',\n",
       " 'cru',\n",
       " 'blomstedt',\n",
       " 'redeem',\n",
       " 'interdisciplinari',\n",
       " 'pamper',\n",
       " 'recreationist',\n",
       " 'choderlo',\n",
       " 'frederico',\n",
       " 'ordeal',\n",
       " 'dykeman',\n",
       " 'gambini',\n",
       " 'showbusi',\n",
       " 'beaumont',\n",
       " 'luminair',\n",
       " 'judah',\n",
       " 'ndergaard',\n",
       " 'brune',\n",
       " 'ziemlich',\n",
       " 'falafel',\n",
       " 'steam',\n",
       " 'sextia',\n",
       " 'mohamad',\n",
       " 'mcfall',\n",
       " 'pamela',\n",
       " 'observ',\n",
       " 'rall',\n",
       " 'kempton',\n",
       " 'desertif',\n",
       " 'irimia',\n",
       " 'tsawwassen',\n",
       " 'emd',\n",
       " 'jasionowka',\n",
       " 'milani',\n",
       " 'ogier',\n",
       " 'tirana',\n",
       " 'nowaday',\n",
       " 'add',\n",
       " 'szaja',\n",
       " 'callofstori',\n",
       " 'kuruvita',\n",
       " 'budhni',\n",
       " 'jagad',\n",
       " 'dweller',\n",
       " 'pentamet',\n",
       " 'mckinnon',\n",
       " 'cantrel',\n",
       " 'porfirio',\n",
       " 'newsag',\n",
       " 'sanema',\n",
       " 'weinglass',\n",
       " 'landrum',\n",
       " 'alka',\n",
       " 'dajjal',\n",
       " 'jajati',\n",
       " 'ruefulli',\n",
       " 'lifeboat',\n",
       " 'blackpool',\n",
       " 'adsorb',\n",
       " 'zaijun',\n",
       " 'oobleck',\n",
       " 'poutndoubt',\n",
       " 'phone',\n",
       " 'vespucci',\n",
       " 'cape',\n",
       " 'harumi',\n",
       " 'cordial',\n",
       " 'elsworth',\n",
       " 'blockbust',\n",
       " 'quigley',\n",
       " 'dru',\n",
       " 'polk',\n",
       " 'vmf',\n",
       " 'froze',\n",
       " 'mockumentri',\n",
       " 'rigg',\n",
       " 'confidant',\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93406809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero():\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5fc1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = range(1, len(set(df['Genre'])) + 1)\n",
    "\n",
    "label_mapping = defaultdict(zero)\n",
    "label_mapping.update({x:y for x, y in zip(set(df['Genre']), labels)})\n",
    "\n",
    "def convert_genre_to_integers(genre):\n",
    "    return label_mapping[genre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db468b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.zero()>,\n",
       "            {'sci-fi': 1,\n",
       "             'musical': 2,\n",
       "             'short': 3,\n",
       "             'animation': 4,\n",
       "             'news': 5,\n",
       "             'horror': 6,\n",
       "             'comedy': 7,\n",
       "             'talk-show': 8,\n",
       "             'action': 9,\n",
       "             'fantasy': 10,\n",
       "             'game-show': 11,\n",
       "             'reality-tv': 12,\n",
       "             'adult': 13,\n",
       "             'crime': 14,\n",
       "             'western': 15,\n",
       "             'biography': 16,\n",
       "             'romance': 17,\n",
       "             'drama': 18,\n",
       "             'sport': 19,\n",
       "             'family': 20,\n",
       "             'mystery': 21,\n",
       "             'war': 22,\n",
       "             'documentary': 23,\n",
       "             'thriller': 24,\n",
       "             'history': 25,\n",
       "             'music': 26,\n",
       "             'adventure': 27})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b8df06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action',\n",
       " 'adult',\n",
       " 'adventure',\n",
       " 'animation',\n",
       " 'biography',\n",
       " 'comedy',\n",
       " 'crime',\n",
       " 'documentary',\n",
       " 'drama',\n",
       " 'family',\n",
       " 'fantasy',\n",
       " 'game-show',\n",
       " 'history',\n",
       " 'horror',\n",
       " 'music',\n",
       " 'musical',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'reality-tv',\n",
       " 'romance',\n",
       " 'sci-fi',\n",
       " 'short',\n",
       " 'sport',\n",
       " 'talk-show',\n",
       " 'thriller',\n",
       " 'war',\n",
       " 'western'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31c2ba43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.449311985833916"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x.split()) for x in df['Description']]) / len(df['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62dae72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Genre'] = df['Genre'].apply(convert_genre_to_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7515019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        18\n",
       "1        24\n",
       "2        13\n",
       "3        18\n",
       "4        18\n",
       "         ..\n",
       "54209     7\n",
       "54210     6\n",
       "54211    23\n",
       "54212     7\n",
       "54213    25\n",
       "Name: Genre, Length: 54214, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bec8ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_length = 64\n",
    "word_value_pool = primes_list[::-1]  # [-500000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5efb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero():\n",
    "    return 0\n",
    "\n",
    "word_val_dict = defaultdict(zero)\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word_val_dict:\n",
    "        word_val_dict[word] = word_value_pool.pop()\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "def make_vectors(sentences_list):\n",
    "    sentence_vectors = []\n",
    "\n",
    "    for sentence in sentences_list:\n",
    "        vec = []\n",
    "        for idx, a_word in enumerate(sentence.strip().split()):\n",
    "            if idx == vector_length:\n",
    "                break\n",
    "\n",
    "            vec.append(word_val_dict[a_word])\n",
    "\n",
    "        vec = vec + [0]*(vector_length - len(vec))\n",
    "\n",
    "        sentence_vectors.append(vec)\n",
    "    \n",
    "    return sentence_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14906362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_vectors = make_vectors( df['Description'] )\n",
    "len(sentence_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc8ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baf9a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Genre\n",
    "X = sentence_vectors\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.001, random_state=143)\n",
    "\n",
    "y_test = df_test['Genre'].apply(convert_genre_to_integers)\n",
    "X_test = make_vectors( df_test.Description )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea4b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c470ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_dict = {\n",
    "#     \"LogisticRegression\": LogisticRegression(multi_class=\"multinomial\", max_iter=500), \n",
    "    \n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "    \"LinearSVC\": LinearSVC(multi_class=\"crammer_singer\"),\n",
    "    \n",
    "#     \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "#     \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "#     \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "#     \"MultinomialNB\": MultinomialNB(),\n",
    "    \n",
    "#     \"GaussianNB\": GaussianNB(),    # # Ram not enough\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1f0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a03b9aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_Models(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    for a_model in all_models_dict:\n",
    "        print(f\"Attempting to Train --> '{a_model}'\")\n",
    "\n",
    "\n",
    "#         scaler_train = preprocessing.StandardScaler().fit(X_train)\n",
    "#         X_test_scaled = scaler_train.transform(X_test)\n",
    "#         X_val_scaled = scaler_train.transform(X_val)\n",
    "#         X_scaled = scaler_train.transform(X_train)\n",
    "#         model = all_models_dict[a_model]\n",
    "#         model.fit(X_scaled, y_train)\n",
    "\n",
    "#         pred_val = model.predict(X_val_scaled)\n",
    "#         pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "        model = all_models_dict[a_model]\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"'{a_model}' is now trained.\")\n",
    "\n",
    "        # We use the predict() on the model to predict the output\n",
    "        print(f\"Predictions for '{a_model}' started.\")\n",
    "        pred_val = model.predict(X_val)\n",
    "        pred_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "        # for classification we use accuracy.\n",
    "        print(f\"Calculating Accuracy of Predictions for '{a_model}'.\")\n",
    "        print(f\"Validation Accuracy for '{a_model}' : {accuracy_score(y_val, pred_val)}\")\n",
    "        print(f\"Test       Accuracy for '{a_model}' : {accuracy_score(y_test, pred_test)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9efe36d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to Train --> 'GradientBoostingClassifier'\n",
      "'GradientBoostingClassifier' is now trained.\n",
      "Predictions for 'GradientBoostingClassifier' started.\n",
      "Calculating Accuracy of Predictions for 'GradientBoostingClassifier'.\n",
      "Validation Accuracy for 'GradientBoostingClassifier' : 0.2909090909090909\n",
      "Test       Accuracy for 'GradientBoostingClassifier' : 0.21957564575645758\n",
      "\n",
      "\n",
      "Attempting to Train --> 'LinearSVC'\n",
      "'LinearSVC' is now trained.\n",
      "Predictions for 'LinearSVC' started.\n",
      "Calculating Accuracy of Predictions for 'LinearSVC'.\n",
      "Validation Accuracy for 'LinearSVC' : 0.0\n",
      "Test       Accuracy for 'LinearSVC' : 0.011918819188191881\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Run_Models(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39dfdfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0347b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a828a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "c1 = 'Human machine interface for Lab ABC computer applications'\n",
    "c2 = 'A survey of user opinion of computer system response time'\n",
    "c3 = 'The EPS user interface management system'\n",
    "c4 = 'System and human system engineering testing of EPS'\n",
    "c5 = 'Relation of user-perceived response time to error measurement'\n",
    "m1 = 'The generation of random, binary, unordered trees'\n",
    "m2 = 'The intersection graph of paths in trees'\n",
    "m3 = 'Graph minors IV: Widths of trees and well-quasi-ordering'\n",
    "m4 = 'Graph minors: A survey'\n",
    "documents = [c1, c2, c3, c4, c5, m1, m2, m3, m4, \"gggg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88686abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer:\n",
    "    vocab = set()\n",
    "    \n",
    "    def create_vectors(self, documents):\n",
    "        clean_docs = list()\n",
    "        \n",
    "        for doc in documents:\n",
    "            lower_doc = doc.lower()\n",
    "            lower_doc_no_sym = ''.join([i if i not in string.punctuation else ' ' for i in lower_doc])\n",
    "            doc_arr = lower_doc_no_sym.split()\n",
    "            clean_docs.append(doc_arr)\n",
    "            self.vocab = self.vocab.union(doc_arr)\n",
    "\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        # Get Counts\n",
    "        vectors = np.zeros((len(clean_docs), len(self.vocab)), dtype=np.int16)\n",
    "        for doc_idx, doc in enumerate(clean_docs):\n",
    "            for wrd_idx, word in enumerate(self.vocab):\n",
    "                count = doc.count(word)\n",
    "                vectors[doc_idx][wrd_idx] = count\n",
    "\n",
    "        return vectors.astype(np.int64)\n",
    "    \n",
    "    \n",
    "    def transform(self, documents):\n",
    "        clean_docs = list()\n",
    "        \n",
    "        for doc in documents:\n",
    "            lower_doc = doc.lower()\n",
    "            lower_doc_no_sym = ''.join([i if i not in string.punctuation else ' ' for i in lower_doc])\n",
    "            doc_arr = lower_doc_no_sym.split()\n",
    "            clean_docs.append(doc_arr)\n",
    "\n",
    "        # Get Counts\n",
    "        vectors = np.zeros((len(clean_docs), len(self.vocab)), dtype=np.int16)\n",
    "        for doc_idx, doc in enumerate(clean_docs):\n",
    "            for wrd_idx, word in enumerate(self.vocab):\n",
    "                count = doc.count(word)\n",
    "                vectors[doc_idx][wrd_idx] = count\n",
    "\n",
    "        return vectors.astype(np.int16)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064b4b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39740664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# By default, CountVectorizer from sklearn ignores tokens under length 2\n",
    "control_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "control_vecs = control_vectorizer.fit_transform(documents)#.toarray()\n",
    "\n",
    "custom_vectorizer = CustomCountVectorizer()\n",
    "custom_vecs = custom_vectorizer.create_vectors(documents)\n",
    "print((control_vecs == custom_vecs).all())\n",
    "\n",
    "# ff = custom_vectorizer.transform(documents)\n",
    "# gg = control_vectorizer.transform(documents).toarray()\n",
    "# print((ff == gg).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83264b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTFIDFVectorizer:\n",
    "  def create_vectors(self, documents):\n",
    "    vocab = set()\n",
    "    clean_docs = list()\n",
    "    for doc in documents:\n",
    "      lower_doc = doc.lower()\n",
    "      lower_doc_no_sym = ''.join([i if i not in string.punctuation else ' ' for i in lower_doc])\n",
    "      doc_arr = lower_doc_no_sym.split()\n",
    "      clean_docs.append(doc_arr)\n",
    "      vocab = vocab.union(doc_arr)\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "\n",
    "    idf_dict = {}\n",
    "    for word in vocab:\n",
    "      count = sum(1 for document in clean_docs if word in document)\n",
    "      idf_dict[word] = 1 + np.log((len(clean_docs) + 1) / (count + 1))\n",
    "\n",
    "    vectors = np.zeros((len(clean_docs), len(vocab)))\n",
    "    for doc_idx, doc in enumerate(clean_docs):\n",
    "      for wrd_idx, word in enumerate(vocab):\n",
    "        tf = doc.count(word) / len(doc)\n",
    "        idf = idf_dict[word]\n",
    "        vectors[doc_idx][wrd_idx] = tf * idf\n",
    "\n",
    "    return preprocessing.normalize(vectors, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3906f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "control_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "control_vecs = control_vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "custom_vectorizer = CustomTFIDFVectorizer()\n",
    "custom_vecs = custom_vectorizer.create_vectors(documents)\n",
    "print(np.allclose(control_vecs, custom_vecs, atol=1e-14))\n",
    "# Due to rounding, we compare for near equality instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe33476",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Genre\n",
    "\n",
    "control_vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = control_vectorizer.fit_transform(df.Description) #.toarray()  ## to save meory\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.001, random_state=143)\n",
    "\n",
    "\n",
    "y_test = df_test['Genre'].apply(convert_genre_to_integers)\n",
    "X_test = control_vectorizer.transform( df_test.Description )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4798154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to Train --> 'GradientBoostingClassifier'\n",
      "'GradientBoostingClassifier' is now trained.\n",
      "Predictions for 'GradientBoostingClassifier' started.\n",
      "Calculating Accuracy of Predictions for 'GradientBoostingClassifier'.\n",
      "Validation Accuracy for 'GradientBoostingClassifier' : 0.43636363636363634\n",
      "Test       Accuracy for 'GradientBoostingClassifier' : 0.5105719557195572\n",
      "\n",
      "\n",
      "Attempting to Train --> 'LinearSVC'\n",
      "'LinearSVC' is now trained.\n",
      "Predictions for 'LinearSVC' started.\n",
      "Calculating Accuracy of Predictions for 'LinearSVC'.\n",
      "Validation Accuracy for 'LinearSVC' : 0.3090909090909091\n",
      "Test       Accuracy for 'LinearSVC' : 0.4942250922509225\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Run_Models(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb1a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75dd2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06753875",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Genre\n",
    "\n",
    "control_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "X = control_vectorizer.fit_transform(df.Description) #.toarray()  ## to save meory\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.001, random_state=143)\n",
    "\n",
    "\n",
    "y_test = df_test['Genre'].apply(convert_genre_to_integers)\n",
    "X_test = control_vectorizer.transform( df_test.Description )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc3dbb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to Train --> 'GradientBoostingClassifier'\n",
      "'GradientBoostingClassifier' is now trained.\n",
      "Predictions for 'GradientBoostingClassifier' started.\n",
      "Calculating Accuracy of Predictions for 'GradientBoostingClassifier'.\n",
      "Validation Accuracy for 'GradientBoostingClassifier' : 0.41818181818181815\n",
      "Test       Accuracy for 'GradientBoostingClassifier' : 0.49939114391143913\n",
      "\n",
      "\n",
      "Attempting to Train --> 'LinearSVC'\n",
      "'LinearSVC' is now trained.\n",
      "Predictions for 'LinearSVC' started.\n",
      "Calculating Accuracy of Predictions for 'LinearSVC'.\n",
      "Validation Accuracy for 'LinearSVC' : 0.41818181818181815\n",
      "Test       Accuracy for 'LinearSVC' : 0.5816974169741698\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Run_Models(X_train, X_val, X_test, y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc890db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
